{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# École Polytechnique de Montréal\n",
    "Département Génie Informatique et Génie Logiciel\n",
    "INF8460 – Traitement automatique de la langue naturelle\n",
    "\n",
    "### Prof. Amal Zouaq\n",
    "### Chargé de laboratoire: Félix Martel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF8460 - TP2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "•\tExplorer les modèles d’espaces vectoriels comme représentations distribuées de la sémantique des mots et des documents\n",
    "\n",
    "•\tComprendre différentes mesures de distance entre vecteurs de documents et de mots\n",
    "\n",
    "•\tUtiliser un modèle de langue n-gramme de caractères et l’algorithme Naive Bayes pour l’analyse de sentiments dans des revues de films (positives, négatives)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prétraitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données est séparé en deux répertoires `train/`et `test`, chacun contenant eux-mêmes deux sous-répertoires `pos/` et `neg/` pour les revues positives et négatives. Un fichier `readme` décrit plus précisément les données.\n",
    "\n",
    "Commencez par lire ces données, en gardant séparées les données d'entraînement et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lire les données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'odre des sections de data: [test_data_pos, test_data_neg, train_data_pos, train_data_neg, train_data_unsup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "    path = \"./data/aclImdb\"\n",
    "    test_endpath = \"/test\"\n",
    "    train_endpath = \"/train\"\n",
    "\n",
    "    files_test_pos = glob.glob(path + test_endpath + \"/pos/*.txt\")\n",
    "    files_test_neg= glob.glob(path + test_endpath + \"/neg/*.txt\")\n",
    "    files_train_pos = glob.glob(path + train_endpath + \"/pos/*.txt\")\n",
    "    files_train_neg = glob.glob(path + train_endpath + \"/neg/*.txt\")\n",
    "    files_train_unsup = glob.glob(path + train_endpath + \"/unsup/*.txt\")\n",
    "    sections = [files_test_pos, files_test_neg, files_train_pos, files_train_neg, files_train_unsup]\n",
    "\n",
    "    #Each data is a list of strings(reviews)\n",
    "    data = [[], [], [], [], []]\n",
    "    for i, section in enumerate(sections):\n",
    "        for file in section:\n",
    "            try:\n",
    "                with open(file, encoding=\"utf8\") as f:\n",
    "                    data[i].append(f.read()) \n",
    "\n",
    "            except IOError as exc:\n",
    "                if exc.errno != errno.EISDIR:\n",
    "                    raise\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données lues\n",
      "Temps d'exécution: 104.65009880065918\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "data = read_dataset()\n",
    "end = time.time()\n",
    "\n",
    "print(\"Données lues\")\n",
    "print(\"Temps d'exécution: \" + str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Créez la fonction `clean_doc()` qui effectue les pré-traitements suivants : segmentation en mots ; \n",
    "suppression des signes de ponctuations ; suppression des mots qui contiennent des caractères autres qu’alphabétiques ; \n",
    "suppression des mots qui sont connus comme des stop words ; suppression des mots qui ont une longueur de 1 caractère. Ensuite, appliquez-la à vos données.\n",
    "\n",
    "Les stop words peuvent être obtenus avec `from nltk.corpus import stopwords`. Vous pourrez utiliser des [expressions régulières](https://docs.python.org/3.7/howto/regex.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess_review(review):\n",
    "    review_no_punct = review.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = nltk.word_tokenize(review_no_punct)\n",
    "\n",
    "    pre_words = []\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word.isalpha() and (word not in stop_words) and len(word) > 1:\n",
    "            pre_words.append(word)\n",
    "\n",
    "    return pre_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(data):\n",
    "    pre_data = [[],[],[],[],[]]\n",
    "    for i, dataset in enumerate(data):\n",
    "        for review in dataset:\n",
    "            pre_data[i].append(preprocess_review(review))\n",
    "    \n",
    "    return pre_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données pré-traitées\n",
      "Temps d'exécution: 94.96665501594543\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "preprocessed_data = clean_doc(data)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Données pré-traitées\")\n",
    "print(\"Temps d'exécution: \" + str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**\tCréez la fonction `build_voc()` qui extrait les unigrammes de l’ensemble d’entraînement et conserve ceux qui ont une fréquence d’occurrence de 5 au moins et imprime le nombre de mots dans le vocabulaire. Sauvegardez-le dans un fichier `vocab.txt` (un mot par ligne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_voc():  \n",
    "    preprocessed_data_train = preprocessed_data[2] + preprocessed_data[3] + preprocessed_data[4]\n",
    "    \n",
    "    counts = defaultdict(int)\n",
    "    for review in preprocessed_data_train:\n",
    "        for unigram in review:\n",
    "            counts[unigram] += 1\n",
    "\n",
    "    unigram = defaultdict(int)\n",
    "    with open(\"vocab.txt\", \"w+\", encoding=\"utf8\") as f:\n",
    "        for count in counts.items():\n",
    "            if (count[1] >= 5):\n",
    "                f.write(count[0] + '\\n')\n",
    "                unigram[count[0]] = count[1]\n",
    "\n",
    "    print(\"Nombre de mots: \" + str(len(unigram)))\n",
    "    return unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mots: 55234\n"
     ]
    }
   ],
   "source": [
    "unigram_train = build_voc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Vous devez créer une fonction `get_top_unigrams(n)` qui retourne les $n$ unigrammes les plus fréquents et les affiche, puis l'appeler avec $n=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_unigrams(n):\n",
    "    sorted_unigram_train = sorted(unigram_train.items(), key=lambda v: v[1], reverse=True)\n",
    "    top_unigrams_train_word = [word[0] for word in sorted_unigram_train]\n",
    "    \n",
    "    return top_unigrams_train_word[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 unigrammes les plus fréquents\n",
      "['br', 'movie', 'film', 'one', 'like', 'good', 'even', 'would', 'time', 'really']\n"
     ]
    }
   ],
   "source": [
    "top_unigrams_train = get_top_unigrams(10)\n",
    "\n",
    "print(\"Top 10 unigrammes les plus fréquents\")\n",
    "print(top_unigrams_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)**\tVous devez créer une fonction `get_top_unigrams_per_cls(n, cls)` qui retourne les $n$ unigrammes les plus fréquents de la classe `cls` (pos ou neg) et les affiche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cls_data_defaultdic(preprocessed_data_per_cls):\n",
    "    counts = defaultdict(int)\n",
    "    for review in preprocessed_data_per_cls:\n",
    "        for unigram in review:\n",
    "            counts[unigram] += 1\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_unigrams_per_cls(n, cls):\n",
    "    if (cls == 'pos'):\n",
    "        preprocessed_data_per_cls = preprocessed_data[2]\n",
    "    elif (cls == 'neg'):\n",
    "        preprocessed_data_per_cls = preprocessed_data[3]\n",
    "    unigram_per_cls = convert_cls_data_defaultdic(preprocessed_data_per_cls)\n",
    "    sorted_unigram_per_cls = sorted(unigram_per_cls.items(), key=lambda v: v[1], reverse=True)\n",
    "    \n",
    "    top_unigrams_per_cls_word = [word[0] for word in sorted_unigram_per_cls]\n",
    "    return top_unigrams_per_cls_word[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)**\tAffichez les 10 unigrammes les plus fréquents dans la classe positive :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 unigrammes les plus fréquents dans la classe positive\n",
      "['br', 'film', 'movie', 'one', 'like', 'good', 'story', 'great', 'time', 'see']\n"
     ]
    }
   ],
   "source": [
    "top_10_unigram_pos = get_top_unigrams_per_cls(10, 'pos')\n",
    "\n",
    "print(\"Top 10 unigrammes les plus fréquents dans la classe positive\")\n",
    "print(top_10_unigram_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)**\tAffichez les 10 unigrammes les plus fréquents dans la classe négative :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 unigrammes les plus fréquents dans la classe négative\n",
      "['br', 'movie', 'film', 'one', 'like', 'even', 'good', 'bad', 'would', 'really']\n"
     ]
    }
   ],
   "source": [
    "top_10_unigram_neg = get_top_unigrams_per_cls(10, 'neg')\n",
    "\n",
    "print(\"Top 10 unigrammes les plus fréquents dans la classe négative\")\n",
    "print(top_10_unigram_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matrices de co-occurence\n",
    "\n",
    "Pour les matrices de cette section, vous pourrez utiliser [des array `numpy`](https://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html) ou des DataFrame [`pandas`](https://pandas.pydata.org/pandas-docs/stable/). \n",
    "\n",
    "Ressources utiles :  le [*quickstart tutorial*](https://numpy.org/devdocs/user/quickstart.html) de numpy et le guide [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Matrice document × mot et TF-IDF\n",
    "\n",
    "\n",
    "Soit $X \\in \\mathbb{R}^{m \\times n}$ une matrice de $m$ documents et $n$ mots, telle que $X_{i,j}$ contient la fréquence d'occurrence du terme $j$ dans le document $i$ :\n",
    "\n",
    "$$\\textbf{rowsum}(X, d) = \\sum_{j=1}^{n}X_{dj}$$\n",
    "\n",
    "$$\\textbf{TF}(X, d, t) = \\frac{X_{d,t}}{\\textbf{rowsum}(X, d)}$$\n",
    "\n",
    "$$\\textbf{IDF}(X, t) = \\log\\left(\\frac{m}{|\\{d : X_{d,t} > 0\\}|}\\right)$$\n",
    "\n",
    "$$\\textbf{TF-IDF}(X, d, t) = \\textbf{TF}(X, d, t) \\cdot \\textbf{IDF}(X, t)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant le même vocabulaire de 5 000 unigrammes, vous devez représenter les documents dans une matrice de co-occurrence document × mot $M(d, w)$  et les pondérer avec la mesure TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première étape est d'avoir le vocabulaire de 5000 unigrammes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_unigram_pos_neg(n):\n",
    "    preprocessed_data_train_pos_neg = preprocessed_data[2] + preprocessed_data[3]\n",
    "    unigram_train_pos_neg = convert_cls_data_defaultdic(preprocessed_data_train_pos_neg)\n",
    "    sorted_unigram_train_pos_neg = sorted(unigram_train_pos_neg.items(), key=lambda v: v[1], reverse=True)\n",
    "    \n",
    "    return sorted_unigram_train_pos_neg[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_top_5000 = get_top_unigram_pos_neg(5000)\n",
    "unigrams_top_5000_words = list(dict(unigrams_top_5000).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_pos_neg = np.asarray(data[2] + data[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous allons construire la matrice bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_BOW(top_unigrams, data_train):\n",
    "    matrix_BOW = np.zeros((data_train.size, len(top_unigrams)))    \n",
    "\n",
    "    for i, review in enumerate(data_train):\n",
    "        pre_review = np.array(preprocess_review(review))\n",
    "        unique, counts = np.unique(pre_review, return_counts = True)\n",
    "        for k, word in enumerate(unique):\n",
    "            if word in top_unigrams:\n",
    "                j = top_unigrams.index(word)\n",
    "                matrix_BOW[i][j] += counts[k]\n",
    "\n",
    "    return matrix_BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice bag of words est finie\n",
      "Temps d'exécution: 149.92942142486572\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "matrix_BOW = count_BOW(unigrams_top_5000_words, data_train_pos_neg)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Matrice bag of words est finie\")\n",
    "print(\"Temps d'exécution: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix_TFIDF(matrix_count_bow, data_train):\n",
    "    sum_cols = np.sum(matrix_count_bow, axis=0)\n",
    "    idf = np.log(data_train.size/sum_cols)\n",
    "    idf = np.absolute(idf)\n",
    "\n",
    "    sum_rows = np.sum(matrix_count_bow, axis=1) \n",
    "    tf = matrix_count_bow / sum_rows.reshape((-1,1))\n",
    "    matrix_TFIDF = tf * idf\n",
    "    \n",
    "    return matrix_TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice TF-IDF est finie\n",
      "Temps d'exécution: 1.5753278732299805\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "matrix_TFIDF = create_matrix_TFIDF(matrix_BOW, data_train_pos_neg)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Matrice TF-IDF est finie\")\n",
    "print(\"Temps d'exécution: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Matrice mot × mot et PPMI (*positive pointwise mutual information*)\n",
    "\n",
    "Vous devez calculer la métrique PPMI. Pour une matrice $m \\times n$ $X$ :\n",
    "\n",
    "\n",
    "$$\\textbf{colsum}(X, j) = \\sum_{i=1}^{m}X_{ij}$$\n",
    "\n",
    "$$\\textbf{sum}(X) = \\sum_{i=1}^{m}\\sum_{j=1}^{n} X_{ij}$$\n",
    "\n",
    "$$\\textbf{expected}(X, i, j) = \n",
    "\\frac{\n",
    "  \\textbf{rowsum}(X, i) \\cdot \\textbf{colsum}(X, j)\n",
    "}{\n",
    "  \\textbf{sum}(X)\n",
    "}$$\n",
    "\n",
    "\n",
    "$$\\textbf{pmi}(X, i, j) = \\log\\left(\\frac{X_{ij}}{\\textbf{expected}(X, i, j)}\\right)$$\n",
    "\n",
    "$$\\textbf{ppmi}(X, i, j) = \n",
    "\\begin{cases}\n",
    "\\textbf{pmi}(X, i, j) & \\textrm{if } \\textbf{pmi}(X, i, j) > 0 \\\\\n",
    "0 & \\textrm{otherwise}\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)**\tA partir des textes du corpus d’entrainement (neg *et* pos), vous devez construire une matrice de co-occurrence mot × mot $M(w,w)$ qui contient les 5000 unigrammes les plus fréquents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matrix_word_word():\n",
    "    matrix_ww = np.zeros((5000, 5000))\n",
    "    dict_unigrams = dict(unigrams_top_5000)\n",
    "    preprocessed_data_pos_neg = np.array(preprocessed_data[2] + preprocessed_data[3])\n",
    "    for review in preprocessed_data_pos_neg:\n",
    "        for i, word in enumerate(review):\n",
    "            if word in dict_unigrams:\n",
    "                index = unigrams_top_5000_words.index(word)\n",
    "                for neighbor in np.concatenate((review[max(0,i-5):i], review[i+1:min(i+6,len(review))])):\n",
    "                    if neighbor in dict_unigrams:\n",
    "                        index_neighbor = unigrams_top_5000_words.index(neighbor)\n",
    "                        matrix_ww[index][index_neighbor] += 1\n",
    "                        matrix_ww[index_neighbor][index] += 1\n",
    "\n",
    "                    \n",
    "    return matrix_ww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de co-occurence terminée\n",
      "Temps d'exécution: 387.26822328567505\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "matrix_ww = calculate_matrix_word_word()\n",
    "end = time.time()\n",
    "\n",
    "print(\"Matrice de co-occurence terminée\")\n",
    "print(\"Temps d'exécution: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.4500e+04 1.7364e+04 1.5606e+04 ... 2.0000e+01 2.6000e+01 1.2000e+01]\n",
      " [1.7364e+04 1.3728e+04 3.7700e+03 ... 1.2000e+01 1.2000e+01 1.6000e+01]\n",
      " [1.5606e+04 3.7700e+03 9.7880e+03 ... 8.0000e+00 1.0000e+01 2.2000e+01]\n",
      " ...\n",
      " [2.0000e+01 1.2000e+01 8.0000e+00 ... 4.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [2.6000e+01 1.2000e+01 1.0000e+01 ... 0.0000e+00 4.0000e+00 0.0000e+00]\n",
      " [1.2000e+01 1.6000e+01 2.2000e+01 ... 0.0000e+00 0.0000e+00 1.2000e+01]]\n"
     ]
    }
   ],
   "source": [
    "print(matrix_ww)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**\tVous devez créer une fonction `calculate_PPMI` qui prend la matrice $M(w,w)$ et la transforme en une matrice $M’(w,w)$ avec les valeurs PPMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_PMI(matrix_ww):\n",
    "    \n",
    "    sum_cols = np.sum(matrix_ww, axis=0)\n",
    "    sum_rows = np.sum(matrix_ww, axis=1) \n",
    "    sum_matrix = np.sum(matrix_ww)\n",
    "    expected = sum_cols.reshape((-1,1)) @ sum_rows.reshape((1,-1)) / sum_matrix\n",
    "    matrix_PMI = np.log2(matrix_ww/expected)\n",
    "    \n",
    "    return matrix_PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_PPMI():\n",
    "    matrix_PMI = calculate_PMI(matrix_ww)\n",
    "    matrix_PPMI = matrix_PMI.clip(0)\n",
    "    \n",
    "    return matrix_PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice PPMI terminée\n",
      "Temps d'exécution: 1.4935357570648193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mohamed ali elakhras\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in log2\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "matrix_PPMI = calculate_PPMI()\n",
    "end = time.time()\n",
    "\n",
    "print(\"Matrice PPMI terminée\")\n",
    "print(\"Temps d'exécution: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.82546523 0.20352149 0.2289324  ... 0.         0.09755701 0.        ]\n",
      " [0.20352149 0.23309692 0.         ... 0.         0.         0.        ]\n",
      " [0.2289324  0.         0.10388032 ... 0.         0.         0.29683612]\n",
      " ...\n",
      " [0.         0.         0.         ... 6.76420194 0.         0.        ]\n",
      " [0.09755701 0.         0.         ... 0.         7.0430781  0.        ]\n",
      " [0.         0.         0.29683612 ... 0.         0.         8.41268956]]\n"
     ]
    }
   ],
   "source": [
    "print(matrix_PPMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mesures de similarité\n",
    "\n",
    "En utilisant le module [scipy.spatial.distance](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html),  définissez des fonctions pour calculer les métriques suivantes :\n",
    "\n",
    "**Distance Euclidienne**\n",
    "\n",
    "La distance euclidienne entre deux vecteurs $u$ et $v$ de dimension $n$ est\n",
    "\n",
    "$$\\textbf{euclidean}(u, v) = \n",
    "\\sqrt{\\sum_{i=1}^{n}|u_{i} - v_{i}|^{2}}$$\n",
    "\n",
    "En deux dimensions, cela correspond à la longueur de la ligne droite entre deux points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Implémentez la fonction `get_euclidean_distance(v1 ,v2)` qui retourne la distance euclidienne entre les vecteurs v1 et v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_euclidean_distance(v1 ,v2):\n",
    "    return distance.euclidean(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance Cosinus**\n",
    "\n",
    "\n",
    "La distance cosinus entre deux vecteurs $u$ et $v$ de dimension $n$ s'écrit :\n",
    "\n",
    "$$\\textbf{cosine}(u, v) = \n",
    "1 - \\frac{\\sum_{i=1}^{n} u_{i} \\cdot v_{i}}{\\|u\\|_{2} \\cdot \\|v\\|_{2}}$$\n",
    "\n",
    "Le terme de droite dans la soustraction mesure l'angle entre $u$ et $v$; on l'appelle la *similarité cosinus* entre $u$ et $v$.\n",
    "\n",
    "**b)** Implémentez la fonction `get_cosinus_distance(v1, v2)` qui retourne la distance cosinus entre les vecteurs v1 et v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosinus_distance(v1, v2):\n",
    "    return distance.cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Implémentez la fonction `get_most_similar_PPMI(word, metric, n)` qui prend un mot en entrée et une mesure de distance et qui retourne les n mots les plus similaires selon la mesure. Les mesures à tester sont : la distance euclidienne et la distance cosinus implantées ci-dessus. Le vecteur du mot word doit être extrait de la matrice $M’(w,w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_PPMI(word, metric, n):\n",
    "    word_unigrams = list(dict(unigrams_top_5000).keys())\n",
    "    index_word = word_unigrams.index(word)\n",
    "    neighbors = matrix_PPMI[index_word]\n",
    "    \n",
    "    distance = {}\n",
    "    for i, word_ww in enumerate(matrix_PPMI):\n",
    "        if metric == \"euclidean\":\n",
    "            distance[word_unigrams[i]] = get_euclidean_distance(neighbors, word_ww)\n",
    "        elif metric == \"cosine\":\n",
    "            distance[word_unigrams[i]] = get_cosinus_distance(neighbors, word_ww)\n",
    "    \n",
    "    sorted_distance = sorted(distance.items(), key=lambda v: v[1], reverse=False)\n",
    "    sorted_distance_word = [word[0] for word in sorted_distance]\n",
    "    \n",
    "    return sorted_distance_word[1:n+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Trouvez les 5 mots les plus similaires au mot « bad » et affichez-les, pour chacune des deux distances. Commentez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 mots les plus similaires au mot bad avec la distance euclidéenne\n",
      "['movie', 'good', 'really', 'even', 'like']\n"
     ]
    }
   ],
   "source": [
    "words_sim_bad = get_most_similar_PPMI(\"bad\", \"euclidean\", 5)\n",
    "\n",
    "print(\"5 mots les plus similaires au mot bad avec la distance euclidéenne\")\n",
    "print(words_sim_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 mots les plus similaires au mot bad avec la distance cosine\n",
      "['acting', 'awful', 'terrible', 'horrible', 'effects']\n"
     ]
    }
   ],
   "source": [
    "words_sim_bad = get_most_similar_PPMI(\"bad\", \"cosine\", 5)\n",
    "\n",
    "print(\"5 mots les plus similaires au mot bad avec la distance cosine\")\n",
    "print(words_sim_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-> Commentez ici<-*\n",
    "\n",
    "La métrique cosine donne de bons résultats. En effet, lorsqu'on regarde les 5 premiers mots, nous avons trois synonymes du mots bad (awful, terrible et horrible). Les deux autres mots ont rapport au mot bad. L'adjectif bad vient ajouter une connotation négative à acting et à effects. On peut conclure que les spectateurs n'ont pas apprécié le jeu d'acteurs et les effets spéciaux. Aussi, on peut voir la proximité des adjectifs awful, terrible et horrible. Cela montre à quel point les spectateurs ont une mauvaise expérience.\n",
    "\n",
    "La métrique euclidéenne donne de moins bons résultats que la distance cosine. En effet, parmis les 5 mots les plus similaire, on retrouve deux antonymes au mot bad (good et like). Aussi, on retrouve le mot even qui a une connotation neutre. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Implémentez la fonction `get_most_similar_TFIDF(word, metric, n)` qui prend un mot en entrée et une mesure de distance et qui retourne les n mots les plus similaires selon la mesure. Les mesures à tester sont : la distance euclidienne et la distance cosinus implantées ci-dessus. Le vecteur du mot word doit être extrait de la matrice $M(d,w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_TFIDF(word, metric, n):\n",
    "    word_unigrams = list(dict(unigrams_top_5000).keys())\n",
    "    index_word = word_unigrams.index(word)\n",
    "    word_documents = matrix_TFIDF[:,index_word]\n",
    "    \n",
    "    distance = {}\n",
    "    for i, word_TFIDF in enumerate(matrix_TFIDF.T):\n",
    "        if metric == \"euclidean\":\n",
    "            distance[word_unigrams[i]] = get_euclidean_distance(word_documents, word_TFIDF)\n",
    "            sorted_distance = sorted(distance.items(), key=lambda v: v[1], reverse=True)\n",
    "        elif metric == \"cosine\":\n",
    "            distance[word_unigrams[i]] = get_cosinus_distance(word_documents, word_TFIDF)\n",
    "            sorted_distance = sorted(distance.items(), key=lambda v: v[1], reverse=False)\n",
    "\n",
    "    sorted_distance_word = [word[0] for word in sorted_distance]\n",
    "\n",
    "    if metric == \"euclidean\":\n",
    "        return sorted_distance_word[:n]\n",
    "    elif metric == \"cosine\":\n",
    "        return sorted_distance_word[1:n+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)** Trouvez les 5 mots les plus similaires au mot « bad » et affichez-les, pour chacune des deux distances. Commentez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['br', 'episode', 'movie', 'show', 'series']\n"
     ]
    }
   ],
   "source": [
    "words_sim_bad = get_most_similar_TFIDF(\"bad\", \"euclidean\", 5)\n",
    "print(words_sim_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'acting', 'good', 'even', 'br']\n"
     ]
    }
   ],
   "source": [
    "words_sim_bad = get_most_similar_TFIDF(\"bad\", \"cosine\", 5)\n",
    "print(words_sim_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-> Commentez ici <-*\n",
    "\n",
    "Cette fois-ci, si l'on compare nos résultats avec ceux de la pondération PPMI, on remarque que nous obtenons l'inverse. La distance euclidéenne donne de meilleurs résultats que la mesure cosine. \n",
    "Pour la distance euclidéenne, on voit clairement que le champ lexical du cinéma est présent avec les mots \"episode\", \"movie\", \"show\" et \"series\". Le mot \"br\" provient du fait que nous retrouvons souvent ce charactère HTML \"< br /> < br />\". Lors de l'étape du prétraitement des données, les caractères de ponctuation seront retirés et, par conséquent, on se retrouve avec br.\n",
    "Pour la distance cosine, les mots sont plus générals. En effet, on voit movie et acting qui sont dans le champ lexical du cinéma. Ensuite, le mots good est un antonyme de bad. Le mot even est à connotation neutre. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification de documents avec un modèle de langue\n",
    "\n",
    "En vous inspirant de [cet article](https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139), entraînez deux modèles de langue $n$-gramme de caractère avec lissage de Laplace, l'un sur le corpus `pos`, l'autre sur le corpus `neg`. Puis, pour chaque document $D$, calculez sa probabilité selon vos deux modèles : $P(D \\mid \\textrm{pos})$ et $P(D \\mid \\textrm{neg})$.\n",
    "\n",
    "Vous pourrez alors prédire sa classe $\\hat{c}_D \\in (\\textrm{pos}, \\textrm{neg})$ en prenant :\n",
    "\n",
    "$$\\hat{c}_D = \\begin{cases}\n",
    "\\textrm{pos} & \\textrm{si } P(D \\mid \\textrm{pos}) > P(D \\mid \\textrm{neg}) \\\\\n",
    "\\textrm{neg} & \\textrm{sinon}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_char_lm(data_train, order=4):\n",
    "    lm = defaultdict(Counter)\n",
    "    dictionnary = defaultdict(lambda: 0)\n",
    "    \n",
    "    pad = \"~\" * (order - 1)\n",
    "    for review in data_train:\n",
    "        review = pad + review\n",
    "        for i in range(len(review)-(order - 1)):\n",
    "            history, char = review[i:i+(order - 1)], review[i+(order - 1)]\n",
    "            dictionnary[char] += 1\n",
    "            lm[history][char] += 1\n",
    "\n",
    "    dictionnary['unk'] = 0\n",
    "    lm['unk']['unk'] = 1 / len(dictionnary)\n",
    "    \n",
    "    def normalize(counter):\n",
    "        s = float(sum(counter.values()))\n",
    "        return {c:(cnt + 1) / (s + len(dictionnary)) for c,cnt in counter.items()}\n",
    "            \n",
    "    \n",
    "    outlm = {hist:normalize(chars) for hist, chars in lm.items()}\n",
    "    return outlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_pos = train_char_lm(data[2], order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_neg = train_char_lm(data[3], order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proba(lm, review, order=4):\n",
    "    proba = 1.0\n",
    "    pad = \"~\" * (order - 1)\n",
    "    review = pad + review\n",
    "    for i in range(len(review)-(order - 1)):\n",
    "        history, char = review[i:i+(order - 1)], review[i+(order - 1)]\n",
    "        try:\n",
    "            proba *= (-1/math.log(lm[history][char]))\n",
    "        except:\n",
    "            proba *= (-1/math.log(lm['unk']['unk']))\n",
    "        \n",
    "    return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification de documents avec sac de mots et Naive Bayes\n",
    "\n",
    "Ici, vous utiliserez l'algorithme Multinomial Naive Bayes (disponible dans [`sklearn.naive_bayes.MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)) pour classifier les documents. Vous utiliserez un modèle sac de mots (en anglais *bag of words*, ou BoW) avec TF-IDF pour représenter vos documents.\n",
    "\n",
    "*Note :* vous avez déjà construit la matrice TF-IDF à la section 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_BOW_Naive():\n",
    "    train_data_Y = np.concatenate((np.ones(len(data[2])), np.zeros(len(data[3]))))\n",
    "    test_data_X = np.asarray(data[0] + data[1])\n",
    "    test_data_Y = np.concatenate((np.ones(len(data[0])), np.zeros(len(data[1]))))\n",
    "    \n",
    "    clf = MultinomialNB(alpha=0.5)\n",
    "    clf.fit(matrix_TFIDF, train_data_Y)    \n",
    "    matrix_TFIDF_test = create_matrix_TFIDF(matrix_BOW,test_data_X)\n",
    "    y_pred = clf.predict(matrix_TFIDF_test)\n",
    "    score = f1_score(test_data_Y, y_pred)\n",
    "                                                  \n",
    "    return score, y_pred, test_data_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_BOW_Naive = classifier_BOW_Naive()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.869164126423873\n"
     ]
    }
   ],
   "source": [
    "print(score_BOW_Naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Améliorations\n",
    "\n",
    "Ici, vous devez proposer une méthode d'amélioration pour le modèle précédent, la justifier et l'implémenter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-> Écrivez vos explications ici <-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un bon moyen serait de rajouter une étape à notre pré-traitement. En effet, si l'on retire tout les mots qui ne sont ni des noms, ni des adjectifs, ni des adverbes et ni des verbes, on garde uniquement les mots qui peuvent décrire une connotation positive ou négative. Cela a aussi pour effet de retirer plusieurs mots du dictionnaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_noun_adj():\n",
    "    new_unigrams_top_5000_words = []\n",
    "    \n",
    "    word_type = nltk.pos_tag(unigrams_top_5000_words, tagset='universal')\n",
    "    for word in word_type:\n",
    "        if word[1] == 'NOUN' or word[1] == 'ADJ' or word[1] == 'ADV' or word[1] == 'VERB':\n",
    "            new_unigrams_top_5000_words.append(word[0])\n",
    "    \n",
    "    return new_unigrams_top_5000_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unigrams_top_5000_words = remove_non_noun_adj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_BOW_amelioration = count_BOW(new_unigrams_top_5000_words, data_train_pos_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_BOW_Naive_Amelioration():\n",
    "    # train_data_X = data[2] + data[3]\n",
    "    train_data_Y = np.concatenate((np.ones(len(data[2])), np.zeros(len(data[3]))))\n",
    "    test_data_X = np.asarray(data[0] + data[1])\n",
    "    test_data_Y = np.concatenate((np.ones(len(data[0])), np.zeros(len(data[1]))))\n",
    "    \n",
    "    matrix_TFIDF_amelioration = create_matrix_TFIDF(matrix_BOW_amelioration, data_train_pos_neg)\n",
    "    clf = MultinomialNB(alpha=0.5)\n",
    "    clf.fit(matrix_TFIDF_amelioration, train_data_Y)    \n",
    "    matrix_TFIDF_test = create_matrix_TFIDF(matrix_BOW_amelioration, test_data_X)\n",
    "    y_pred = clf.predict(matrix_TFIDF_test)\n",
    "    score = f1_score(test_data_Y, y_pred)\n",
    "                                                  \n",
    "    return score, y_pred, test_data_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_BOW_Naive_Amelioration = classifier_BOW_Naive_Amelioration()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8695582458109518\n"
     ]
    }
   ],
   "source": [
    "print(score_BOW_Naive_Amelioration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Évaluation\n",
    "\n",
    "Évaluation des modèles des sections 4, 5, 6 sur les données de test. On attend les métriques suivantes : *accuracy*, et pour chaque classe précision, rappel, score F1. Vous pourrez utiliser le module [`sklearn.metrics`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_reviews_prob():\n",
    "    test_data_X = np.asarray(data[0] + data[1])\n",
    "    test_data_Y = np.concatenate((np.ones(len(data[0])), np.zeros(len(data[1]))))\n",
    "    \n",
    "    probas = []\n",
    "    for review in test_data_X:\n",
    "        proba_pos = get_proba(lm_pos, review)\n",
    "        proba_neg = get_proba(lm_neg, review)\n",
    "        if (proba_pos > proba_neg):\n",
    "            probas.append(1)\n",
    "        else:\n",
    "            probas.append(0)\n",
    "    \n",
    "    return probas, test_data_Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.88      0.81     12500\n",
      "         1.0       0.86      0.71      0.77     12500\n",
      "\n",
      "    accuracy                           0.79     25000\n",
      "   macro avg       0.80      0.79      0.79     25000\n",
      "weighted avg       0.80      0.79      0.79     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_sec4, y_true_sec4 = get_all_reviews_prob()\n",
    "print('Section 4')\n",
    "print(classification_report(y_true_sec4, y_pred_sec4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.87      0.87     12500\n",
      "         1.0       0.87      0.87      0.87     12500\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.87      0.87      0.87     25000\n",
      "weighted avg       0.87      0.87      0.87     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_sec5, y_pred_sec5, y_true_sec5 = classifier_BOW_Naive()\n",
    "print('Section 5')\n",
    "print(classification_report(y_true_sec5, y_pred_sec5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.87      0.87     12500\n",
      "         1.0       0.87      0.87      0.87     12500\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.87      0.87      0.87     25000\n",
      "weighted avg       0.87      0.87      0.87     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_sec6, y_pred_sec6, y_true_sec6 = classifier_BOW_Naive_Amelioration()\n",
    "print('Section 6')\n",
    "print(classification_report(y_true_sec6, y_pred_sec6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commentez vos résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-> Commentez ici vos résultats <-*\n",
    "La valeur de 0.0 correspond à \"négatif\" et la valeur de 1.0 correspond à \"positif\"\n",
    "\n",
    "Nous avons testé nos résultats dans les trois cas. Dans le premier cas, nous obtenons 12580 valeurs positives et 12420 valeurs négatives. Si nous affichons les valeurs du vecteur y_pred_sec4, on peut voir que les valeurs sont réparties de manière quasiment aléatoire. Cela explique la raison pour laquelle nous obtenons 0.50.\n",
    "\n",
    "Pour les sections 5 et 6, on peut voir que nos valeurs sont beaucoup plus précises. En effet, nous avons affiché les valeurs du vecteur y_pred_sec5 et on peut voir que nos 12500 premières valeurs (les reviews positives) contiennent 10835 valeurs positives (environ 87% de succés). Dans les 12500 autres valeurs, on obtient 10903 valeurs négatives (environ 87% de succés). \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
